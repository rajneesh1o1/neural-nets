{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "-qSErc5THj-9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "import torch\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "X0ccwI71e18r"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "  # builds a set of all nodes and edges in a graph\n",
        "  nodes, edges = set(), set()\n",
        "  def build(v):\n",
        "    if v not in nodes:\n",
        "      nodes.add(v)\n",
        "      for child in v._prev:\n",
        "        edges.add((child, v))\n",
        "        build(child)\n",
        "  build(root)\n",
        "  return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
        "\n",
        "  nodes, edges = trace(root)\n",
        "  for n in nodes:\n",
        "    uid = str(id(n))\n",
        "    # for any value in the graph, create a rectangular ('record') node for it\n",
        "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
        "    if n._op:\n",
        "      # if this value is a result of some operation, create an op node for it\n",
        "      dot.node(name = uid + n._op, label = n._op)\n",
        "      # and connect this node to it\n",
        "      dot.edge(uid + n._op, uid)\n",
        "\n",
        "  for n1, n2 in edges:\n",
        "    # connect n1 to the op node of n2\n",
        "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "  return dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "P1uFNOWAa5Ec"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, _children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0\n",
        "    self._backward = lambda: None\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "\n",
        "  def __add__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "    out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "    def _backward():\n",
        "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __rmul__(self, other): # other * self\n",
        "    return self * other\n",
        "\n",
        "  def __truediv__(self, other): # self / other\n",
        "    return self * other**-1\n",
        "\n",
        "  def __neg__(self): # -self\n",
        "    return self * -1\n",
        "\n",
        "  def __sub__(self, other): # self - other\n",
        "    return self + (-other)\n",
        "\n",
        "  def __radd__(self, other): # other + self\n",
        "    return self + other\n",
        "\n",
        "  def tanh(self):\n",
        "    x = self.data\n",
        "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
        "    out = Value(t, (self, ), 'tanh')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 - t**2) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def exp(self):\n",
        "    x = self.data\n",
        "    out = Value(math.exp(x), (self, ), 'exp')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += out.data * out.grad # NOTE: in the video I incorrectly used = instead of +=. Fixed here.\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "  def backward(self):\n",
        "\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "gBOOi5VbalA1"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, nin):\n",
        "    self.w =[Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "    self.b = Value(random.uniform(-1,1))\n",
        "  def __call__(self, x):\n",
        "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
        "    out = act.tanh()\n",
        "    return out\n",
        "  def parameters(self):\n",
        "    return self.w + [self.b]\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, nin, nout):\n",
        "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "  def __call__(self, x):\n",
        "    outs = [n(x) for n in self.neurons]\n",
        "    return outs\n",
        "  def parameters(self):\n",
        "    return [p for neurons in self.neurons for p in neurons.parameters()]\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, nin, nouts):\n",
        "    sz = [nin] + nouts    # add the input dimension as first layer and after that each layer will have previous layer n neurons output as n dimentional input , the last layer will have only one neuron so that we can have one prediction it can also have mutiple outputs but here it iwll be one.\n",
        "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x= layer(x)\n",
        "    return x;\n",
        "  def parameters(self):\n",
        "    return [p for layers in self.layers for p in layers.parameters()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeM-3lPCiOkE",
        "outputId": "6f0d7f9e-59f0-48e7-f9e5-89d42e2e6757"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[Value(data=-0.98431892135704)],\n",
              " [Value(data=-0.5732852958296546)],\n",
              " [Value(data=-0.5121756537721991)],\n",
              " [Value(data=-0.9774799820136655)]]"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xs = [\n",
        "    # [1.11, 1.12, -1.13]\n",
        "    [2.0,3.0,-1.0],\n",
        "    [3.0,-1.0,0.5],\n",
        "    [0.5,1.0, 1.0],\n",
        "    [1.0,1.0,-1.0]\n",
        "]\n",
        "n= MLP(3,[4,4,1])\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired outout\n",
        "\n",
        "yprid = [n(x) for x in xs]\n",
        "yprid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-mgTF4AknG5",
        "outputId": "73c5d8a8-5fd6-4df2-f02b-b61093b3e0b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Value(data=8.2680066924481)"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = sum(((yout[0]-ygt)*(yout[0]-ygt)) for ygt, yout in zip(ys, yprid))\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "z66GdzxSl7fn"
      },
      "outputs": [],
      "source": [
        "loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXCSgI11l_BI",
        "outputId": "326f8fcf-77d0-4c3f-992b-2ee22b3a57f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3733616509205048"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n.layers[0].neurons[0].w[0].grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nve91PtuMMT",
        "outputId": "fad7f809-4b2c-44e1-8295-8e2f77cfaffb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.8217314466125483"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n.layers[0].neurons[0].w[0].data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTGmsrq2y4vE"
      },
      "source": [
        "this slight chnage in weight improvs the the loss dramatically:\n",
        "forward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGrr1u-Iyb9a",
        "outputId": "21d2e18a-b7cf-43f9-df88-c04ac46727d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8.2680066924481\n",
            "[[Value(data=-0.98431892135704)], [Value(data=-0.5732852958296546)], [Value(data=-0.5121756537721991)], [Value(data=-0.9774799820136655)]]\n",
            "[1.0, -1.0, -1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "yprid = [n(x) for x in xs]\n",
        "loss = sum(((yout[0]-ygt)*(yout[0]-ygt)) for ygt, yout in zip(ys, yprid))\n",
        "print(loss.data)\n",
        "print(yprid)\n",
        "print(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "tkdMgYOry98W"
      },
      "outputs": [],
      "source": [
        "# now run the backword pass to update the gradients it self.\n",
        "loss.backward()\n",
        "#!/bin/bash\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4LlQux0u3Yz"
      },
      "source": [
        "update:\n",
        "\n",
        "now changing grad little bit to minimise the loss\n",
        "the logic for negative here is\n",
        "if let's say grad is +ve so increasing the weight will increase the loss so we need to decreae the weights so need -ve sign\n",
        "\n",
        "if let's say grad is -ve so we wanna increase the weight so that loss can decrease hence we need to make -ve grad sign to +ve hence again -ve *sign*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "YVYSiZcWmlDl"
      },
      "outputs": [],
      "source": [
        "for p in n.parameters():\n",
        "  p.data +=-0.01 * p.grad \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uq_hzjSm1O_6",
        "outputId": "8be47b79-8e99-416a-e27d-c995b116e593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value(data=-0.8291986796309584)\n",
            "Value(data=-0.3806013410950249)\n",
            "Value(data=0.33062924235889446)\n",
            "Value(data=-0.027131659271635973)\n",
            "Value(data=-0.9759320384222411)\n",
            "Value(data=0.4688687525563479)\n",
            "Value(data=-0.898296093468187)\n",
            "Value(data=-0.6057774796670697)\n",
            "Value(data=0.12104942655216483)\n",
            "Value(data=0.21139648345406584)\n",
            "Value(data=-0.41286242960004793)\n",
            "Value(data=-0.286076676911969)\n",
            "Value(data=0.2573747642752236)\n",
            "Value(data=-0.6129534820127742)\n",
            "Value(data=0.6216204827277477)\n",
            "Value(data=0.47886871654244)\n",
            "Value(data=0.7599730581778152)\n",
            "Value(data=-0.5884523889784035)\n",
            "Value(data=-0.8921611064447186)\n",
            "Value(data=-0.03373260348612301)\n",
            "Value(data=-0.33922731320253996)\n",
            "Value(data=-0.9878497903318578)\n",
            "Value(data=0.6502602856839987)\n",
            "Value(data=-0.6348041812018224)\n",
            "Value(data=-0.6078100133249322)\n",
            "Value(data=0.9418641872717076)\n",
            "Value(data=0.5662951140439263)\n",
            "Value(data=0.3324218240643566)\n",
            "Value(data=0.7306365882930318)\n",
            "Value(data=-0.9820590947204811)\n",
            "Value(data=0.8786451458717488)\n",
            "Value(data=0.7891147484691594)\n",
            "Value(data=0.7845571113433112)\n",
            "Value(data=0.020393384227796636)\n",
            "Value(data=0.7990596948496242)\n",
            "Value(data=0.3219967164128261)\n",
            "Value(data=0.3551457107911424)\n",
            "Value(data=-0.9521023815846819)\n",
            "Value(data=-0.5842452540116492)\n",
            "Value(data=0.3459640472744592)\n",
            "Value(data=-0.3511331363333555)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for p in n.parameters():\n",
        "  print(p)\n",
        "len(n.parameters())\n",
        "# draw_dot(n(x)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Loss: 0.010245129847984046\n",
            "Step 1, Loss: 0.010214000490325134\n",
            "Step 2, Loss: 0.010183052368281945\n",
            "Step 3, Loss: 0.010152283931589895\n",
            "Step 4, Loss: 0.01012169364742532\n",
            "Step 5, Loss: 0.010091280000163055\n",
            "Step 6, Loss: 0.01006104149113747\n",
            "Step 7, Loss: 0.010030976638407702\n",
            "Step 8, Loss: 0.010001083976526864\n",
            "Step 9, Loss: 0.009971362056314758\n",
            "Converged at step 9!\n",
            "Value(data=0.009971362056314758)\n",
            "[[Value(data=0.9603439718143755)], [Value(data=-0.9583808657096059)], [Value(data=-0.9440614231714686)], [Value(data=0.9405232417815804)]]\n",
            "[1.0, -1.0, -1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "for k in range(10):  # max 100 iterations\n",
        "\n",
        "  # forward pass\n",
        "  yprid = [n(x) for x in xs]\n",
        "  loss = sum(((yout[0]-ygt)*(yout[0]-ygt)) for ygt, yout in zip(ys, yprid))\n",
        "  \n",
        "  for p in n.parameters():\n",
        "    p.grad = 0.0\n",
        "\n",
        "  # Backward pass\n",
        "  loss.backward()\n",
        "  \n",
        "  print(f\"Step {k}, Loss: {loss.data}\")\n",
        "\n",
        "  for p in n.parameters():\n",
        "    p.data += -0.01 * p.grad\n",
        "  \n",
        "  \n",
        "  if loss.data < 0.01:\n",
        "    print(f\"Converged at step {k}!\")\n",
        "    break\n",
        "\n",
        "print(loss)\n",
        "print(yprid)\n",
        "print(ys)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
